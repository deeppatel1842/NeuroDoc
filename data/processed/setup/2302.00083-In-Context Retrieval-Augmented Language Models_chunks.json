[
  {
    "text": "In-Context Retrieval-Augmented Language Models Ori Ram Yoav Levine Itay Dalmedigos Dor Muhlgay Amnon Shashua Kevin Leyton-Brown Yoav Shoham AI21 Labs  orir,yoavl,itayd,dorm,amnons,kevinlb,yoavs  ai21. com Abstract Retrieval-Augmented Language Modeling (RALM) methods, which condition a lan- guage model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve lan- guage modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natu- ral source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the in- corporation of external information, signifi- cantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM : leaving the LM architec- ture unchanged and prepending grounding documents to the input, without any further training of the LM.",
    "chunk_size": 952,
    "word_count": 128,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 0,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_0",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "paper considers a simple alternative, which we dub In-Context RALM : leaving the LM architec- ture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse cor- pora. We also demonstrate that the document retrieval and ranking mechanism can be spe- cialized to the RALM setting to further boost performance. We conclude that In-Context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access. 1 1 Introduction Recent advances in language modeling (LM) have dramatically increased the usefulness of machine- generated text across a wide range of use-cases and domains (Brown et al. , 2020).",
    "chunk_size": 908,
    "word_count": 140,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 1,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_1",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "1 1 Introduction Recent advances in language modeling (LM) have dramatically increased the usefulness of machine- generated text across a wide range of use-cases and domains (Brown et al. , 2020). However, the mainstream paradigm of generating text with LMs bears inherent limitations in access to external knowledge. First, LMs are not coupled with any  Equal contribution. 1Our code is available at https:  github. com  AI21Labs in-context-ralm Perplexity 10. 015. 020. 025. 030. O GPT-2 345M (M) GPT-2 1. 5B (XL)No Retrieval In-Context RALM (BM25) In-Context RALM (Predictive Reranking)Figure 1: Our framework, dubbed In-Context RALM , provides large language modeling gains on the test set of WikiText-103, without modifying the LM. Adapting the use of a BM25 retriever (Robert- son and Zaragoza, 2009) to the LM task ( 5) yields significant gains, and choosing the grounding doc- uments via our new class of Predictive Rerankers ( 6) provides a further boost.",
    "chunk_size": 964,
    "word_count": 151,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 2,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_2",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "(Robert- son and Zaragoza, 2009) to the LM task ( 5) yields significant gains, and choosing the grounding doc- uments via our new class of Predictive Rerankers ( 6) provides a further boost. See Table 1 for the full results on five diverse corpora. source attribution, and must be trained in order to incorporate up-to-date information that was not seen during training. More importantly, they tend to produce factual inaccuracies and errors (Lin et al. , 2022; Maynez et al. , 2020; Huang et al. , 2020). This problem is present in any LM gen- eration scenario, and is exacerbated when gener- ation is made in uncommon domains or private data. A promising approach for addressing the above is Retrieval-Augmented Language Modeling (RALM), grounding the LM during generation by conditioning on relevant documents retrieved from an external knowledge source.",
    "chunk_size": 857,
    "word_count": 139,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 3,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_3",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "for addressing the above is Retrieval-Augmented Language Modeling (RALM), grounding the LM during generation by conditioning on relevant documents retrieved from an external knowledge source. RALM systems in- clude two high level components: (i) document se- lection , selecting the set of documents upon which to condition; and (ii) document reading , determin- ing how to incorporate the selected documents into the LM generation process. Leading RALM systems introduced recentlyarXiv:2302. 00083v3  cs. CL  1 Aug 2023 Language Model World Cup 2022 was the last with 32 teams, before the increase to Retriever FIFA World Cup 2026 will expand to 48 teams. World Cup 2022 was the last with 32 teams, before the increase to 48 in the 2026 tournament. Figure 2: An example of In-Context RALM : we simply prepend the retrieved document before the input prefix. tend to be focused on altering the language model architecture (Khandelwal et al. , 2020; Borgeaud et al. , 2022; Zhong et al.",
    "chunk_size": 984,
    "word_count": 160,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 4,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_4",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": " RALM : we simply prepend the retrieved document before the input prefix. tend to be focused on altering the language model architecture (Khandelwal et al. , 2020; Borgeaud et al. , 2022; Zhong et al. , 2022; Levine et al. , 2022c; Li et al. , 2022). Notably, Borgeaud et al. (2022) in- troduced RETRO, featuring document reading via nontrivial modifications that require further train- ing to the LM architecture, while using an off-the- shelf frozen BERT retriever for document selec- tion. Although the paper's experimental findings showed impressive performance gains, the need for changes in architecture and dedicated retraining has hindered the wide adoption of such models. In this paper, we show that a very simple doc- ument reading mechanism can have a large im- pact, and that substantial gains can also be made by adapting the document selection mechanism to the task of language modeling.",
    "chunk_size": 902,
    "word_count": 147,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 5,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_5",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "that a very simple doc- ument reading mechanism can have a large im- pact, and that substantial gains can also be made by adapting the document selection mechanism to the task of language modeling. Thus, we show that many of the benefits of RALM can be achieved while working with off-the-shelf LMs, even via API access. Specifically, we consider a simple but powerful RALM framework, dubbed In-Context RALM (presented in Section 3), which employs a zero-effort document reading mechanism: we sim- ply prepend the selected documents to the LM's input text (Figure 2). Section 4 describes our experimental setup. To show the wide applicability of our framework, we performed LM experiments on a suite of five di- verse corpora: WikiText-103 (Merity et al. , 2016), RealNews (Zellers et al. , 2019), and three datasets from The Pile (Gao et al. , 2021): ArXiv, Stack Exchange and FreeLaw.",
    "chunk_size": 886,
    "word_count": 147,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 6,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_6",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "on a suite of five di- verse corpora: WikiText-103 (Merity et al. , 2016), RealNews (Zellers et al. , 2019), and three datasets from The Pile (Gao et al. , 2021): ArXiv, Stack Exchange and FreeLaw. We use open-source LMs ranging from 110M to 66B parameters (from the GPT-2, GPT-Neo, OPT and LLaMA model fami- lies). In Section 5 we evaluate the application of off- the-shelf retrievers to our framework. In this minimal-effort setting, we found that In-Context RALM led to LM performance gains equivalent to increasing the LM's number of parameters by 2  3 across all of the text corpora we examined. In Section 6 we investigate methods for adapting doc-ument ranking to the LM task, a relatively under- explored RALM degree of freedom. Our adapta- tion methods range from using a small LM to per- form zero-shot ranking of the retrieved documents, up to training a dedicated bidirectional reranker by employing self-supervision from the LM signal.",
    "chunk_size": 948,
    "word_count": 158,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 7,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_7",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": " tion methods range from using a small LM to per- form zero-shot ranking of the retrieved documents, up to training a dedicated bidirectional reranker by employing self-supervision from the LM signal. These methods lead to further gains in the LM task corresponding to an additional size increase of 2  in the LM architecture. As a concrete example of the gains, a 345M parameter GPT-2 enhanced by In-Context RALM outperforms a 762M parame- ter GPT-2 when employing an off-the-shelf BM25 retriever (Robertson and Zaragoza, 2009), and out- performs a 1. 5B parameter GPT-2 when employing our trained LM-oriented reranker (see Figure 1). For large model sizes, our method is even more effective: In-Context RALM with an off-the-shelf retriever improved the performance of a 6. 7B pa- rameter OPT model to match that of a 66B param- eter parameter OPT model (see Figure 4). In Section 7 we demonstrate the applicability of In-Context RALM to downstream open-domain questions answering (ODQA) tasks.",
    "chunk_size": 995,
    "word_count": 159,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 8,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_8",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "to match that of a 66B param- eter parameter OPT model (see Figure 4). In Section 7 we demonstrate the applicability of In-Context RALM to downstream open-domain questions answering (ODQA) tasks. In a concurrent work, Shi et al. (2023) also sug- gest to augment off-the-shelf LMs with retrieved texts by prepending them to the input. Their re- sults are based on training a dedicated retriever for language modeling. In contrast, we focus on the gains achievable in using off-the-shelf retrievers for this task. We show strong gains of this simpler setting by investigating: (1) which off-the-shelf retriever is best suited for language modeling, (2) the frequency of retrieval operations, and (3) the optimal query length. In addition, we boost the off- the-shelf retrieval performance by introducing two reranking methods that demonstrate further gains in perplexity. We believe that In-Context RALM can play two important roles in making RALM systems more powerful and more prevalent.",
    "chunk_size": 987,
    "word_count": 154,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 9,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_9",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "introducing two reranking methods that demonstrate further gains in perplexity. We believe that In-Context RALM can play two important roles in making RALM systems more powerful and more prevalent. First, given its simple reading mechanism, In-Context RALM can serve as a clean probe for developing document retrieval methods that are specialized for the LM task. These in turn can be used to improve both In-Context RALM and other more elaborate RALM methods that currently leverage general purpose retrievers. Second, due to its compatibility with off-the-shelf LMs, In-Context RALM can help drive wider de- ployment of RALM systems. 2 Related Work RALM approaches can be roughly divided into two families of models: (i) nearest-neighbor language models (also called kNN-LM), and (ii) retrieve and read models. Our work belongs to the second family, but is distinct in that it involves no further training of the LM.",
    "chunk_size": 918,
    "word_count": 144,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 10,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_10",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": " (i) nearest-neighbor language models (also called kNN-LM), and (ii) retrieve and read models. Our work belongs to the second family, but is distinct in that it involves no further training of the LM. Nearest Neighbor Language Models ThekNN- LM approach was first introduced in Khandel- wal et al. (2020). The authors suggest a simple inference-time model that interpolates between two next-token distributions: one induced by the LM itself, and one induced by the kneighbors from the retrieval corpus that are closest to the query token in the LM embedding space. Zhong et al. (2022) sug- gest a framework for training these models. While they showed significant gains from kNN-LM, the approach requires storing the representations for each token in the corpus , an expensive requirement even for a small corpus like Wikipedia. Although numerous approaches have been suggested for al- leviating this issue (He et al. , 2021; Alon et al.",
    "chunk_size": 937,
    "word_count": 151,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 11,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_11",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "token in the corpus , an expensive requirement even for a small corpus like Wikipedia. Although numerous approaches have been suggested for al- leviating this issue (He et al. , 2021; Alon et al. , 2022), scaling any of them to large corpora remains an open challenge. Retrieve and Read Models This family of RALMs creates a clear division between document selection anddocument reading components. All prior work involves training the LM. We begin by describing works that use this approach for tack- ling downstream tasks, and then mention works ori- ented towards RALM. Lewis et al. (2020) and Izac- ard and Grave (2021) fine tuned encoder decoder architectures for downstream knowledge-intensive tasks. Izacard et al. (2022b) explored different ways of pretraining such models, while Levine et al. (2022c) pretrained an autoregressive LM on clusters of nearest neighbors in sentence embed- ding space. Levine et al.",
    "chunk_size": 919,
    "word_count": 146,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 12,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_12",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "(2022b) explored different ways of pretraining such models, while Levine et al. (2022c) pretrained an autoregressive LM on clusters of nearest neighbors in sentence embed- ding space. Levine et al. (2022a) showed competi- tive open domain question-answering performance by prompt-tuning a frozen LM as a reader. Guu et al. (2020) pretrained REALM, a retrieval aug- mented bidirectional, masked LM, later fine-tunedfor open-domain question answering. The work closest to this paper with a focus on the language modeling task is RETRO (Borgeaud et al. , 2022), which modifies an autoregressive LM to attend to relevant documents via chunked cross-attention, thus introducing new parameters to the model. Our In-Context RALM differs from prior work in this family of models in two key aspects: •We use off-the-shelf LMs for document read- ingwithout any further training of the LM. •We focus on how to choose documents for improved LM performance. 3 Our Framework 3.",
    "chunk_size": 963,
    "word_count": 150,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 13,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_13",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": " models in two key aspects: •We use off-the-shelf LMs for document read- ingwithout any further training of the LM. •We focus on how to choose documents for improved LM performance. 3 Our Framework 3. 1 In-Context RALM Language models define probability distributions over sequences of tokens. Given such a sequence x1,. , x n, the standard way to model its probabil- ity is via next-token prediction: p(x1,. , x n)  Qn i 1p(xi x i), where x i: x1,. , x i 1is the sequence of tokens preceding xi, also referred to as its prefix. This autoregressive model is usu- ally implemented via a learned transformer net- work (Vaswani et al. , 2017) parameterized by the set of parameters θ: p(x1,. , x n)  nY i 1pθ(xi x i), (1) where the conditional probabilities are modeled by employing a causal self-attention mask (Rad- ford et al. , 2018). Notably, leading LMs such as GPT-2 (Radford et al. , 2019), GPT-3 (Brown et al. , 2020), OPT (Zhang et al. , 2022) or Jurassic- 1 (Lieber et al.",
    "chunk_size": 980,
    "word_count": 177,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 14,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_14",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "causal self-attention mask (Rad- ford et al. , 2018). Notably, leading LMs such as GPT-2 (Radford et al. , 2019), GPT-3 (Brown et al. , 2020), OPT (Zhang et al. , 2022) or Jurassic- 1 (Lieber et al. , 2021) follow this simple parame- terization. Retrieval augmented language models (RALMs) add an operation that retrieves one or more docu- ments from an external corpus C, and condition the above LM predictions on these documents. Specifi- cally, for predicting xi, the retrieval operation from Cdepends on its prefix: RC(x i), so the most general RALM decomposition is: p(x1,. , x n)  Qn i 1p(xi x i,RC(x i)). In order to condition the LM generation on the retrieved document, pre- vious RALM approaches used specialized architec- tures or algorithms (see  2). Inspired by the suc- cess of In-Context Learning (Brown et al. , 2020; Dong et al.",
    "chunk_size": 845,
    "word_count": 145,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 15,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_15",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "on the retrieved document, pre- vious RALM approaches used specialized architec- tures or algorithms (see  2). Inspired by the suc- cess of In-Context Learning (Brown et al. , 2020; Dong et al. , 2023), In-Context RALM refers to the following specific, simple method of concatenating the retrieved documents2within the Transformer's input prior to the prefix (see Figure 2), which does not involve altering the LM weights θ: p(x1,. , x n)   nY i 1pθ(xi  RC(x i);x i ),(2) where  a;b denotes the concatenation of strings a andb. Since common Transformer-based LM imple- mentations support limited length input sequences, when the concatenation of the document and the input sequence exceed this limit we remove to- kens from the beginning of xuntil the overall input length equals that allowed by the model. Because our retrieved documents are passages of limited length, we always have enough context left from x (see  4. 3). 3.",
    "chunk_size": 928,
    "word_count": 151,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 16,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_16",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "of xuntil the overall input length equals that allowed by the model. Because our retrieved documents are passages of limited length, we always have enough context left from x (see  4. 3). 3. 2 RALM Design Choices We detail below two practical design choices often made in RALM systems. In  5, we investigate the effect of these in the setting of In-Context RALM. Retrieval Stride While in the above formulation a retrieval operation can occur at each generation step, we might want to perform retrieval only once every s  1tokens due to the cost of calling the retriever, and the need to replace the documents in the LM prefix during generation. We refer to sas theretrieval stride. This gives rise to the follow- ing In-Context RALM formulation (which reduces back to Eq. (2) for s  1): p(x1,. , x n)   ns 1Y j 0sY i 1pθ  xs j i   RC(x s j);x (s j i)   , (3) where ns n sis the number of retrieval strides.",
    "chunk_size": 907,
    "word_count": 166,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 17,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_17",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "ing In-Context RALM formulation (which reduces back to Eq. (2) for s  1): p(x1,. , x n)   ns 1Y j 0sY i 1pθ  xs j i   RC(x s j);x (s j i)   , (3) where ns n sis the number of retrieval strides. Notably, in this framework the runtime costs of each retrieval operation is composed of (a) apply- ing the retriever itself, and (b) recomputing the embeddings of the prefix. In  5. 2 we show that us- ing smaller retrieval strides, i. e. , retrieving as often as possible, is superior to using larger ones (though In-Context RALM with larger strides already pro- vides large gains over vanilla LM). Thus, choosing the retrieval stride is ultimately a tradeoff between runtime and performance. 2We always use a single document , but it is conceptually simple to support multiple documents as well. Retrieval Query Length While the retrieval query above in principle depends on all prefix to- kensx s j, the information at the very end of the prefix is typically the most relevant to the generated tokens.",
    "chunk_size": 997,
    "word_count": 177,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 18,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_18",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "Query Length While the retrieval query above in principle depends on all prefix to- kensx s j, the information at the very end of the prefix is typically the most relevant to the generated tokens. If the retrieval query is too long then this in- formation can be diluted. To avoid this, we restrict the retrieval query at stride jto the last ℓtokens of the prefix, i. e. , we use qs,ℓ j: xs j ℓ 1,. , x s j. We refer to ℓas the retrieval query length. Note that prior RALM work couples the retrieval stride sand the retrieval query length ℓ(Borgeaud et al. , 2022). In  5, we show that enforcing s ℓdegrades LM performance. Integrating these hyper-parameters into the In-Context RALM formulation gives p(x1,. , x n)   ns 1Y j 0sY i 1pθ  xs j i h RC(qs,ℓ j);x (s j i)i. (4) 4 Experimental Details We now describe our experimental setup, including all models we use and their implementation details. 4.",
    "chunk_size": 900,
    "word_count": 166,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 19,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_19",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "p(x1,. , x n)   ns 1Y j 0sY i 1pθ  xs j i h RC(qs,ℓ j);x (s j i)i. (4) 4 Experimental Details We now describe our experimental setup, including all models we use and their implementation details. 4. 1 Datasets We evaluated the effectiveness of In-Context RALM across five diverse language modeling datasets and two common open-domain question answering datasets. Language Modeling The first LM dataset is WikiText-103 (Merity et al. , 2016), which has been extensively used to evaluate RALMs (Khandelwal et al. , 2020; He et al. , 2021; Borgeaud et al. , 2022; Alon et al. , 2022; Zhong et al. , 2022). Second, we chose three datasets spanning diverse subjects from The Pile (Gao et al. , 2021): ArXiv ,Stack Exchange andFreeLaw. Finally, we also investigated Real- News (Zellers et al. , 2019), since The Pile lacks a corpus focused only on news (which is by nature a knowledge-intensive domain).",
    "chunk_size": 897,
    "word_count": 155,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 20,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_20",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": ",Stack Exchange andFreeLaw. Finally, we also investigated Real- News (Zellers et al. , 2019), since The Pile lacks a corpus focused only on news (which is by nature a knowledge-intensive domain). Open-Domain Question Answering In order to evaluate In-Context RALM on downstream tasks as well, we use the Natural Questions (NQ; Kwiatkowski et al. 2019) and TriviaQA (Joshi et al. , 2017) open-domain question answering datasets. 4. 2 Models Language Models We performed our experi- ments using the four models of GPT-2 (110M  1. 5B; Radford et al. 2019), three models of GPT- Neo and GPT-J (1. 3B 6B; Black et al. 2021; Wang and Komatsuzaki 2021), eight models of OPT (125M 66B; Zhang et al. 2022) and three mod- els of LLaMA (7B 33B; Touvron et al. 2023). All models are open source and publicly available. 3 We elected to study these particular models for the following reasons. The first four (GPT-2) mod- els were trained on WebText (Radford et al.",
    "chunk_size": 951,
    "word_count": 162,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 21,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_21",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "All models are open source and publicly available. 3 We elected to study these particular models for the following reasons. The first four (GPT-2) mod- els were trained on WebText (Radford et al. , 2019), with Wikipedia documents excluded from their training datasets. We were thus able to evaluate our method's \"zero-shot\" performance when retrieving from a novel corpus (for WikiText-103). The rest of the models brought two further benefits. First, they allowed us to investigate how our methods scale to models larger than GPT-2. Second, the fact that Wikipedia was part of their training data allowed us to investigate the usefulness of In-Context RALM for corpora seen during training. The helpfulness of such retrieval has been demonstrated for previ- ous RALM methods (Khandelwal et al. , 2020) and has also been justified theoretically by Levine et al. (2022c).",
    "chunk_size": 870,
    "word_count": 139,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 22,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_22",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "during training. The helpfulness of such retrieval has been demonstrated for previ- ous RALM methods (Khandelwal et al. , 2020) and has also been justified theoretically by Levine et al. (2022c). We ran all models with a maximum sequence length of 1,024, even though GPT-Neo, OPT and LLaMA models support a sequence length of 2,048. 4 Retrievers We experimented with both sparse (word-based) and dense (neural) retrievers. We used BM25 (Robertson and Zaragoza, 2009) as our sparse model. For dense models, we experimented with (i) a frozen BERT-base (Devlin et al. , 2019) followed by mean pooling, similar to Borgeaud et al. (2022); and (ii) the Contriever (Izacard et al. , 2022a) and Spider (Ram et al. , 2022) models, which are dense retrievers that were trained in un- supervised manners. Reranking When training rerankers (Sec- tion 6. 2), we initialized from RoBERTa-base (Liu et al. , 2019). 4.",
    "chunk_size": 902,
    "word_count": 149,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 23,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_23",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": ", 2022) models, which are dense retrievers that were trained in un- supervised manners. Reranking When training rerankers (Sec- tion 6. 2), we initialized from RoBERTa-base (Liu et al. , 2019). 4. 3 Implementation Details We implemented our code base using the Trans- formers library (Wolf et al. , 2020). We based our dense retrieval code on the DPR repository (Karpukhin et al. , 2020). 3All models are available for use use via https:   huggingface. co  4In preliminary experiments, we observed similar improve- ments from In-Context RALM when using a sequence length of 2,048. We used a sequence length of 1,024 in order to facilitate a direct comparison between all models. Perplexity 10203040 GPT-2 117M (S) GPT-2 1. 5B (XL)No Retrieval BERT Contriever Spider BM25Figure 3: The performance of four off-the-shelf retrievers used for In-Context RALM on the de- velopment set of WikiText-103. All RALMs are run with s  4(i. e. , retrieval is applied every four tokens).",
    "chunk_size": 972,
    "word_count": 158,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 24,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_24",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "3: The performance of four off-the-shelf retrievers used for In-Context RALM on the de- velopment set of WikiText-103. All RALMs are run with s  4(i. e. , retrieval is applied every four tokens). For each RALM, we report the result of the best query length ℓ(see Figures 6, 9, 10). Retrieval Corpora For WikiText-103 and ODQA datasets, we used the Wikipedia corpus from Dec. 20, 2018, standardized by Karpukhin et al. (2020) using the preprocessing from Chen et al. (2017). To avoid contamination, we found and removed all 120 articles of the development and test set of WikiText-103 from the corpus. For the remaining datasets, we used their training data as the retrieval corpus. Similar to Karpukhin et al. (2020), our retrieval corpora consist of non-overlapping passages of 100 words (which translate to less than 150 tokens for the vast majority of passages). Thus, we truncate our retrieved passages at 256 tokens when input to the models, but they are usually much smaller.",
    "chunk_size": 981,
    "word_count": 163,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 25,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_25",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "words (which translate to less than 150 tokens for the vast majority of passages). Thus, we truncate our retrieved passages at 256 tokens when input to the models, but they are usually much smaller. Retrieval For sparse retrieval, we used the Py- serini library (Lin et al. , 2021). For dense retrieval, we applied exact search using FAISS (Johnson et al. , 2021). 5 The Effectiveness of In-Context RALM with Off-the-Shelf Retrievers We now empirically show that despite its simple document reading mechanism, In-Context RALM leads to substantial LM gains across our diverse evaluation suite. We begin in this section by inves- tigating the effectiveness of off-the-shelf retrievers for In-Context RALM; we go on in  6 to show that further LM gains can be made by tailoring document ranking functions to the LM task. The experiments in this section provided us with a recommended configuration for applying In- Model Retrieval RerankingWikiText-103 RealNews ArXiv Stack Exch.",
    "chunk_size": 975,
    "word_count": 155,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 26,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_26",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "ranking functions to the LM task. The experiments in this section provided us with a recommended configuration for applying In- Model Retrieval RerankingWikiText-103 RealNews ArXiv Stack Exch. FreeLaw word ppl token ppl token ppl token ppl token ppl GPT-2 S    37. 5 21. 3 12. O 12. 8 13. O BM25  5   29. 6 16. 1 10. 9 11. 3 9. 6 BM25 Zero-shot  6. 1 28. 6 15. 5 10. 1 10. 6 8. 8 BM25 Predictive  6. 2 26. 8         GPT-2 M    26. 3 15. 7 9. 3 8. 8 9. 6 BM25  5   21. 5 12. 4 8. 6 8. 1 7. 4 BM25 Zero-shot  6. 1 20. 8 12. O 8. O 7. 7 6. 9 BM25 Predictive  6. 2 19. 7         GPT-2 I    22. O 13. 6 8. 4 8. 5 8. 7 BM25  5   18. 1 10. 9 7. 8 7. 8 6. 8 BM25 Zero-shot  6. 1 17. 6 10. 6 7. 3 7. 4 6. 4 BM25 Predictive  6. 2 16. 6         GPT-2 XL    20. O 12. 4 7. 8 8. O 8. O BM25  5   16. 6 10. 1 7. 2 7. 4 6. 4 BM25 Zero-shot  6. 1 16. 1 9. 8 6. 8 7. 1 6. O BM25 Predictive  6. 2 15. 4         Table 1: Perplexity on the test set of WikiText-103, RealNews and three datasets from the Pile.",
    "chunk_size": 988,
    "word_count": 230,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 27,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_27",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": " 10. 1 7. 2 7. 4 6. 4 BM25 Zero-shot  6. 1 16. 1 9. 8 6. 8 7. 1 6. O BM25 Predictive  6. 2 15. 4         Table 1: Perplexity on the test set of WikiText-103, RealNews and three datasets from the Pile. For each LM, we report: (a) its performance without retrieval, (b) its performance when fed the top-scored passage by BM25 ( 5), and (c) its performance when applied on the top-scored passage of each of our two suggested rerankers ( 6). All models share the same vocabulary, thus token-level perplexity ( token ppl ) numbers are comparable. For WikiText we follow prior work and report word-level perplexity ( word ppl ). Model RetrievalWikiText-103 word ppl LLaMA-7B- 9. 9 BM25,  5 8. 8 LLaMA-13B- 8. 5 BM25,  5 7. 6 LLaMA-33B- 6. 3 BM25,  5 6. 1 Table 2: The performance of models from the LLaMA family, measured by word-level perplexity on the test set of WikiText-103. Context RALM: applying a sparse BM25 retriever that receives ℓ  32 query tokens and is applied as frequently as possible.",
    "chunk_size": 995,
    "word_count": 180,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 28,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_28",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "family, measured by word-level perplexity on the test set of WikiText-103. Context RALM: applying a sparse BM25 retriever that receives ℓ  32 query tokens and is applied as frequently as possible. Practically, we retrieve every s  4 tokens ( ℓandsare defined in  3). Table 1 shows for the GPT-2 models that across all the examined corpora, employing In-Context RALM with an off-the-shelf retriever improved LM perplexity to a sufficient extent that it matched that of a 2 3 larger model. Figure 4 and Tables 2 and 5 show that this trend holds across model sizes up to 66B parameters, for both WikiText-103 andRealNews. 5. 1 BM25 Outperforms Off-the-Shelf Neural Retrievers in Language Modeling We experimented with different off-the-shelf gen- eral purpose retrievers, and found that the sparse (lexical) BM25 retriever (Robertson and Zaragoza, 2009) outperformed three popular dense (neu- ral) retrievers: the self-supervised retrievers Con- triever (Izacard et al. , 2022a) and Spider (Ram et al.",
    "chunk_size": 998,
    "word_count": 156,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 29,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_29",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "BM25 retriever (Robertson and Zaragoza, 2009) outperformed three popular dense (neu- ral) retrievers: the self-supervised retrievers Con- triever (Izacard et al. , 2022a) and Spider (Ram et al. , 2022), as well as a retriever based on the average pooling of BERT embeddings that was used in the RETRO system (Borgeaud et al. , 2022). We conducted a minimal hyper-parameter search on the query length ℓfor each of the retrievers, and found that ℓ  32 was optimal for BM25 (Fig- ure 6), and ℓ  64 worked best for dense retrievers (Figures 9, 10). Figure 3 compares the performance gains of In- Context RALM with these four general-purpose re- trievers. The BM25 retriever clearly outperformed all dense retrievers. This outcome is consistent with prior work showing that BM25 outperforms neural retrievers across a wide array of tasks, when applied in zero-shot settings (Thakur et al. , 2021). This result renders In-Context RALM even more WikiText-103Perplexity 10. 020. 030. 040.",
    "chunk_size": 980,
    "word_count": 158,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 30,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_30",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "neural retrievers across a wide array of tasks, when applied in zero-shot settings (Thakur et al. , 2021). This result renders In-Context RALM even more WikiText-103Perplexity 10. 020. 030. 040. O OPT-125M OPT-350M OPT-1. 3B OPT-2. 7B OPT-6. 7B OPT-13B OPT-30B OPT-66BNo Retrieval In-Context RALM (BM25) RealNewsPerplexity 3. 08. 013. 018. O OPT-125M OPT-350M OPT-1. 3B OPT-2. 7B OPT-6. 7B OPT-13B OPT-30B OPT-66BNo Retrieval In-Context RALM (BM25)Figure 4: Results of OPT models (Zhang et al. , 2022) on the test set of WikiText-103 (word-level perplexity) and the development set of RealNews (token-level perplexity). In-Context RALM models use a BM25 retriever with s  4(i. e. , the retriever is called every four tokens) and ℓ  32 (i. e. , the retriever query is comprised of the last 32 tokens of the prefix). In-Context RALM with an off-the-shelf retriever improved the performance of a 6. 7B parameter OPT model to match that of a 66B parameter OPT model.",
    "chunk_size": 962,
    "word_count": 156,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 31,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_31",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "is comprised of the last 32 tokens of the prefix). In-Context RALM with an off-the-shelf retriever improved the performance of a 6. 7B parameter OPT model to match that of a 66B parameter OPT model. appealing since applying a BM25 retriever is sig- nificantly cheaper than the neural alternatives. 5. 2 Frequent Retrieval Improves Language Modeling We investigated the effect of varying the retrieval stride s(i. e. , the number of tokens between consec- utive retrieval operations). Figure 5 shows that LM performance improved as the retrieval operation became more frequent. This supports the intuition that retrieved documents become more relevant the closer the retrieval query becomes to the gener- ated tokens. Of course, each retrieval operation imposes a runtime cost. To balance performance and runtime, we used s  4 in our experiments. For comparison, RETRO employed a retrieval fre- quency of s  64 (Borgeaud et al. , 2022), which leads to large degradation in perplexity.",
    "chunk_size": 983,
    "word_count": 156,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 32,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_32",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "performance and runtime, we used s  4 in our experiments. For comparison, RETRO employed a retrieval fre- quency of s  64 (Borgeaud et al. , 2022), which leads to large degradation in perplexity. Intuitively, retrieving with high frequency (low retrieval stride) allows to ground the LM in higher resolution. 5. 3 A Contextualization vs. Recency Tradeoff in Query Length We also investigated the effect of varying ℓ, the length of the retrieval query for BM25. Figure 6 reveals an interesting tradeoff and a sweet spot around a query length of 32tokens. Similar ex- periments for dense retrievers are given in App. A. We conjecture that when the retriever query is too short, it does not include enough of the input con- text, decreasing the retrieved document's relevance. Conversely, excessively growing the retriever query deemphasizes the tokens at the very end of the pre- fix, diluting the query's relevance to the LM task.",
    "chunk_size": 929,
    "word_count": 152,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 33,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_33",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "the retrieved document's relevance. Conversely, excessively growing the retriever query deemphasizes the tokens at the very end of the pre- fix, diluting the query's relevance to the LM task. 6 Improving In-Context RALM with LM-Oriented Reranking Since In-Context RALM uses a fixed document reading component by definition, it is natural to ask whether performance can be improved by spe- cializing its document retrieval mechanism to the LM task. Indeed, there is considerable scope for improvement: the previous section considered con- ditioning the model only on the first document re- Retrieval Stride ( 𝑠)Perplexity 10. 020. 030. 040. O 1 2 4 8 16 32 64GPT-2 117M (S) GPT-2 345M (M) GPT-2 762M (I) GPT-2 1. 5B (XL)Figure 5: An analysis of perplexity as a function ofs, the retrieval stride ,i. e. , the number of tokens between consecutive retrieval operations, on the development set of WikiText-103. Throughout the paper, we use s  4 to balance perplexity and runtime.",
    "chunk_size": 975,
    "word_count": 158,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 34,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_34",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "retrieval stride ,i. e. , the number of tokens between consecutive retrieval operations, on the development set of WikiText-103. Throughout the paper, we use s  4 to balance perplexity and runtime. Retrieval Query Length (ℓ)Perplexity 10. 015. 020. 025. 030. 035. O 16 32 64GPT-2 117M (S) GPT-2 345M (M) GPT-2 762M (I) GPT-2 1. 5B (XL) Figure 6: An analysis of perplexity as a function ofthe number of tokens in the query ℓfor BM25 on the development set of WikiText-103. In the appendix, we show similar trade-offs for dense retrievers within WikiText-103. Throughout the paper, we use a query length of ℓ  32 tokens. trieved by the BM25 retriever. This permits very limited semantic understanding of the query, since BM25 is based only on the bag of words signal. Moreover, it offers no way to accord different de- grees of importance to different retrieval query to- kens, such as recognizing that later query tokens are more relevant to the generated text.",
    "chunk_size": 960,
    "word_count": 162,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 35,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_35",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "Moreover, it offers no way to accord different de- grees of importance to different retrieval query to- kens, such as recognizing that later query tokens are more relevant to the generated text. In this section, we focus on choosing which doc- ument to present to the model, by reranking the top-kdocuments returned by the BM25 retriever. 5 We use Figure 7 as motivation: it shows the large potential for improvement among the top- 16docu- ments returned by the BM25 retriever. We act upon 5In both  6. 1 and  6. 2 we use k  16. Perplexity 10. 020. 030. 040. O No Retrieval BM25 (Top-1) Oracle: BM25 (Top-16)GPT-2 117M (S) GPT-2 345M (M) GPT-2 762M (I) GPT-2 1. 5B (XL)Figure 7: Potential for gains from reranking: per- plexity improvement (on the development set of WikiText-103) from an oracle that takes the best of the top-16 documents retrieved by BM25 rather than the first. this motivation by using two rerankers. Specifi- cally, in  6.",
    "chunk_size": 943,
    "word_count": 162,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 36,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_36",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": " the development set of WikiText-103) from an oracle that takes the best of the top-16 documents retrieved by BM25 rather than the first. this motivation by using two rerankers. Specifi- cally, in  6. 1 we show performance gains across our evaluation suite obtained by using an LM to perform zero-shot reranking of the top- kBM25 retrieved documents (results in third row for each of the models in Table 1). Then, in  6. 2 we show that training a specialized bidirectional reranker of the top- kBM25 retrieved documents in a self- supervised manner via the LM signal can provide further LM gains (results in forth row for each of the models in Table 1). 6. 1 LMs as Zero-Shot Rerankers First, we used off-the-shelf language models as document rerankers for the In-Context RALM set- ting. Formally, for a query qconsisting of the lastℓtokens in the prefix of the LM input x, let  d1,. , d k be the top- kdocuments returned by BM25. For retrieval iteration j, let the text for generation be y: xs j 1,.",
    "chunk_size": 1000,
    "word_count": 176,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 37,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_37",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "a query qconsisting of the lastℓtokens in the prefix of the LM input x, let  d1,. , d k be the top- kdocuments returned by BM25. For retrieval iteration j, let the text for generation be y: xs j 1,. , x s j s. Ideally, we would like to find the document di that maximizes the probability of the text for generation, i. e. , i   arg max i  k pθ(y  di;x s j ). (5) However, at test time we do not have access to the tokens of y. Instead, we used the last pre- fixtokens (which areavailable at test time), de- noted by y , for reranking. Formally, let s be a hyper-parameter that determines the number of the prefix tokens by which to rerank. We define y : xs j s  1,. , x s j(i. e. , the stride of length s  that precedes y) and choose the document dˆisuch ModelReranking ModelWikiText-103 RealNews word ppl token ppl GPT-2 345M (M)GPT-2 110M (S) 20. 8 12. 1 GPT-2 345M (M) 20. 8 12. O GPT-2 762M (I)GPT-2 110M (S) 17. 7 10. 7 GPT-2 762M (I) 17. 6 10. 6 GPT-2 1. 5B (XL)GPT-2 110M (S) 16. 2 9.",
    "chunk_size": 991,
    "word_count": 203,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 38,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_38",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "word ppl token ppl GPT-2 345M (M)GPT-2 110M (S) 20. 8 12. 1 GPT-2 345M (M) 20. 8 12. O GPT-2 762M (I)GPT-2 110M (S) 17. 7 10. 7 GPT-2 762M (I) 17. 6 10. 6 GPT-2 1. 5B (XL)GPT-2 110M (S) 16. 2 9. 9 GPT-2 1. 5B (XL) 16. 1 9. 8 Table 3: Perplexity for zero-shot reranking ( 6. 1) where the reranking models is smaller than the LM, or the LM itself. Reranking is performed on the top 16 documents retrieved by BM25. Using a GPT-2 110M (S) instead of a larger language model as a reranker leads to only a minor degradation. that ˆi  arg max i  k pϕ(y    di;x (s j s )  ). (6) The main motivation is that since BM25 is a lexical retriever, we want to incorporate a semantic signal induced by the LM. Also, this reranking shares con- ceptual similarities with the reranking framework of Sachan et al. (2022) for open-domain question answering, where y (i. e. , the last prefix tokens) can be thought of as their \"question\".",
    "chunk_size": 916,
    "word_count": 179,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 39,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_39",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "con- ceptual similarities with the reranking framework of Sachan et al. (2022) for open-domain question answering, where y (i. e. , the last prefix tokens) can be thought of as their \"question\". Note that our zero-shot reranking does not re- quire that the LM used for reranking is the same model as the LM used for generation ( i. e. , the LM in Eq. (6), parameterized by ϕ, does not need to be the LM in Eq. (2), parameterized by θ). This ob- servation unlocks the possibility of reranking with smaller (and thus faster) models, which is impor- tant for two main reasons: (i) Reranking kdocu- ments requires kforward passes; and (ii) it allows our methods to be used in cases where the actual LM's log probabilities are not available (for exam- ple, when the LM is accessed through an API). 6 Results A minimal hyper-parameter search on the development set of WikiText-103 revealed that the optimal query length is s   16 ,7so we proceed with this value going forward.",
    "chunk_size": 970,
    "word_count": 171,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 40,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_40",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "through an API). 6 Results A minimal hyper-parameter search on the development set of WikiText-103 revealed that the optimal query length is s   16 ,7so we proceed with this value going forward. Table 1 shows the results of letting the LM perform zero-shot rerank- ing on the top-16 documents retrieved by BM25 (third row for each of the models). It is evident that reranking yielded consistently better results than simply taking the first result returned by the retriever. 6Note we do not require that the two models share the same vocabulary. 7We experimented with s    4,8,16,32. Table 3 shows that a small LM (GPT-2 117M) can be used to rerank the documents for all larger GPT-2 models, with roughly the same performance as having each LM perform reranking for itself, supporting the applicability of this method for LMs that are only accessible via an API. 6. 2 Training LM-dedicated Rerankers Next, we trained a reranker to choose one of the top-kdocuments retrieved by BM25.",
    "chunk_size": 982,
    "word_count": 164,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 41,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_41",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "the applicability of this method for LMs that are only accessible via an API. 6. 2 Training LM-dedicated Rerankers Next, we trained a reranker to choose one of the top-kdocuments retrieved by BM25. We refer to this approach as Predictive Reranking , since the reranker learns to choose which document will help in \"predicting\" the upcoming text. For this process, we assume availability of training data from the target corpus. Our reranker is a classifier that gets a prefix x s jand a document di(fori  k ), and produces a scalar f(x s j, di)that should resemble the relevance of diforthe continuation ofx s j. We then normalize these relevance scores: prank(di x s j)  exp(f(x s j, di))Pk i  1exp(f(x s j, di )),(7) and choose the document dˆisuch that ˆi  arg max i  k prank(di x s j). (8) Collecting Training Examples To train our pre- dictive reranker, we collected training examples as follows. Let x s jbe a prefix we sample from the training data, and y: xs j 1,.",
    "chunk_size": 972,
    "word_count": 173,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 42,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_42",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": " prank(di x s j). (8) Collecting Training Examples To train our pre- dictive reranker, we collected training examples as follows. Let x s jbe a prefix we sample from the training data, and y: xs j 1,. , x s j sbe the text for generation upcoming in its next stride. We run BM25 on the query qs,ℓ jderived from x s j (see  3. 2) and get kdocuments  d1,. , d k. For each document di, we then run the LM to compute pθ(y  di;x s j )similar to Eq. (4). Number of documentsEM O. 020. 040. 060. 080. O O 1 2 3 4TriviaQA: LLaMa-7B TriviaQA: LLaMa-13B NQ: LLaMa-7B NQ: LLaMa-13BFigure 8: Zero-shot performance of In-Context RALM on the development set of Natural Ques- tions and TriviaQA, when varying the number of documents (retrieved by DPR) shown in-context. Training Our reranker was a fine-tuned RoBERTa-base (Liu et al. , 2019) that trained for 10,000 steps with a peak learning rate of 10 5and a batch size of 32.",
    "chunk_size": 912,
    "word_count": 170,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 43,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_43",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "(retrieved by DPR) shown in-context. Training Our reranker was a fine-tuned RoBERTa-base (Liu et al. , 2019) that trained for 10,000 steps with a peak learning rate of 10 5and a batch size of 32. Overall, we created 300,000 examples from the training set of WikiText-103 as explained above. The loss function we use to train the reranker follows previous work (Guu et al. , 2020; Lewis et al. , 2020):  logkX i 1prank(di x s j) pθ(y  di;x s j ). (9) Note that unlike those works, we train only the reranker ( prank), keeping the LM weights θfrozen. Results Table 1 shows the result of our predictive reranker, trained on WikiText-103. Specifically, we trained it with data produced by GPT-2 110M (S), and tested its effectiveness for all GPT-2 models. We observed significant gains obtained from Predic- tive Reranking. For example, the perplexity of GPT- 2 110M (S) improved from 29. 6 to 26. 8, and that of GPT-2 1. 5B (XL) improved from 16. 6 to 15. 4.",
    "chunk_size": 955,
    "word_count": 171,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 44,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_44",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "significant gains obtained from Predic- tive Reranking. For example, the perplexity of GPT- 2 110M (S) improved from 29. 6 to 26. 8, and that of GPT-2 1. 5B (XL) improved from 16. 6 to 15. 4. This trend held for the other two models as well. Overall, these results demonstrate that training a reranker with domain-specific data was more effective than zero-shot reranking (Section 6. 1). Note that these results while impressive still leave room for fur- ther improvements, compared to the top-16 BM25 oracle results (see Figure 7). Moreover, the oracle results themselves can be improved by retrieving k   16documents via a BM25 retriever, or by training stronger retrievers dedicated to the RALM task. We leave this direction for future work. Model Retrieval NQ TriviaQA LLaMA-7B- 10. 3 47. 5 DPR 28. O 56. O LLaMA-13B- 12. O 54. 8 DPR 31. O 60. 1 LLaMA-33B- 13. 7 58. 3 DPR 32. 3 62.",
    "chunk_size": 886,
    "word_count": 156,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 45,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_45",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "the RALM task. We leave this direction for future work. Model Retrieval NQ TriviaQA LLaMA-7B- 10. 3 47. 5 DPR 28. O 56. O LLaMA-13B- 12. O 54. 8 DPR 31. O 60. 1 LLaMA-33B- 13. 7 58. 3 DPR 32. 3 62. 7 Table 4: Zero-shot results of In-Context RALM on the test set of Natural Questions and TriviaQA mea- sured by exact match. In the open-book setting, we include the top two documents returned by DPR. 7 In-Context RALM for Open-Domain Question Answering So far, we evaluated our framework on language modeling benchmarks. To test its efficacy in addi- tional scenarios, and specifically downstream tasks, we now turn to evaluate In-Context RALM on open- domain question answering (ODQA; Chen et al. 2017). This experiment is intended to verify, in a controlled environment, that LMs can leverage retrieved documents without further training and without any training examples. Specifically, we use the LLaMA family (Touvron et al.",
    "chunk_size": 927,
    "word_count": 157,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 46,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_46",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "to verify, in a controlled environment, that LMs can leverage retrieved documents without further training and without any training examples. Specifically, we use the LLaMA family (Touvron et al. , 2023) with andwithout In-Context RALM (often referred to in ODQA literature as open-book and closed-book settings, respectively). In contrast to most prior work on ODQA ( e. g. , Izacard and Grave 2021; Fa- jcik et al. 2021; Izacard et al. 2022b; Levine et al. 2022b), our \"reader\" ( i. e. , the model that gets the question along with its corresponding retrieved doc- uments, and returns the answer) is simply a frozen large LM: notpretrained, fine-tuned or prompted to be retrieval-augmented. For the closed-book set- ting, we utilize the prompt of Touvron et al. (2023). For the open-book setting, we extend this prompt to include retrieved documents (see App. C). We use DPR (Karpukhin et al. , 2020) as our retriever.",
    "chunk_size": 920,
    "word_count": 152,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 47,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_47",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "we utilize the prompt of Touvron et al. (2023). For the open-book setting, we extend this prompt to include retrieved documents (see App. C). We use DPR (Karpukhin et al. , 2020) as our retriever. Varying the Number of Documents To inves- tigate the the effect of the number of documents shown to the model, we performed a minimal anal- ysis on the development set of NQ and TriviaQA. Figure 8 demonstrates that showing documents in- context significantly improves the model's perfor- mance. In addition, most of the gain can be ob- tained by using only two documents (or even a single one in some cases). Results Table 4 gives the results of In-Context RALM on the test set of Natural Questions and TriviaQA. Motivated by our previous findings, we used two retrieved documents. It is evident that showing the model relevant documents sig- nificantly boosted its performance. For example, adding retrieved documents improved LLaMA- 13B in the zero-shot setting by more than 18 points on NQ (from 12.",
    "chunk_size": 999,
    "word_count": 169,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 48,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_48",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "the model relevant documents sig- nificantly boosted its performance. For example, adding retrieved documents improved LLaMA- 13B in the zero-shot setting by more than 18 points on NQ (from 12. O  to 31. O ) and more than 5 points on TriviaQA (from 54. 8  to 60. 1 ). 8 Discussion Retrieval from external sources has become a com- mon practice in knowledge-intensive tasks (such as factual question answering, fact checking, and more; Petroni et al. 2021). In parallel, recent break- throughs in LM generation capabilities has led to LMs that can generate useful long texts. How- ever, factual inaccuracies remain a common way in which machine-generated text can fall short, and lack of direct provenance makes it hard to trust machine generated text. This makes language mod- eling both a promising and an urgent new applica- tion area for knowledge grounding, and motivates promoting RALM approaches. Prior research has already investigated RALM, of course, but it is not yet widely deployed.",
    "chunk_size": 994,
    "word_count": 162,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 49,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_49",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "and an urgent new applica- tion area for knowledge grounding, and motivates promoting RALM approaches. Prior research has already investigated RALM, of course, but it is not yet widely deployed. One likely reason is that existing approaches rely upon fine-tuning the LM, which is typically difficult and costly, and is even impossible for LMs accessible only via an API. This paper presented the framework of In- Context RALM , enabling frozen, off-the-shelf LMs to benefit from retrieval. We demonstrated that substantial performance gains can be achieved by using general purpose retrievers, and showed that additional gains can be achieved by tailoring the document selection to the LM setting. A recent work by Muhlgay et al. (2023) demonstrates that In-Context RALM is indeed able to improve the factuality of large LMs. Several directions for further improvement re- main for future work.",
    "chunk_size": 894,
    "word_count": 140,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 50,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_50",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "A recent work by Muhlgay et al. (2023) demonstrates that In-Context RALM is indeed able to improve the factuality of large LMs. Several directions for further improvement re- main for future work. First, this paper considers only the case of prepending a single external docu- ment to the context; adding more documents could drive further gains (for example, using the frame- work of Ratner et al. 2022). Second, we retrieved documents every fixed interval of stokens, but see potential for large latency and cost gains by retriev- ing more sparsely, such as only when a specialized model predicts that retrieval is needed. We release the code used in this work, for thecommunity to use and improve over. We hope it will drive further research of RALM, which will enable its wider adoption. Acknowledgements We would like to thank the reviewers and the Ac- tion Editor for their valuable feedback. References Uri Alon, Frank Xu, Junxian He, Sudipta Sengupta, Dan Roth, and Graham Neubig. 2022.",
    "chunk_size": 994,
    "word_count": 164,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 51,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_51",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "We would like to thank the reviewers and the Ac- tion Editor for their valuable feedback. References Uri Alon, Frank Xu, Junxian He, Sudipta Sengupta, Dan Roth, and Graham Neubig. 2022. Neuro- symbolic language modeling with automaton- augmented retrieval. In ICML. Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. Sebastian Borgeaud, Arthur Mensch, Jordan Hoff- mann, Trevor Cai, Eliza Rutherford, Katie Mil- lican, George Bm Van Den Driessche, Jean- Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irv- ing, Oriol Vinyals, Simon Osindero, Karen Si- monyan, Jack Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by re- trieving from trillions of tokens. In ICML. Tom B.",
    "chunk_size": 958,
    "word_count": 143,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 52,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_52",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": " Geoffrey Irv- ing, Oriol Vinyals, Simon Osindero, Karen Si- monyan, Jack Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by re- trieving from trillions of tokens. In ICML. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems. Danqi Chen, Adam Fisch, Jason Weston, and An- toine Bordes. 2017. Reading Wikipedia to an- swer open-domain questions.",
    "chunk_size": 894,
    "word_count": 127,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 53,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_53",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "are few-shot learners. In Advances in Neural Information Processing Systems. Danqi Chen, Adam Fisch, Jason Weston, and An- toine Bordes. 2017. Reading Wikipedia to an- swer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1870 1879, Vancouver, Canada. Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Con- ference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, Volume 1 (Long and Short Papers) , pages 4171 4186, Minneapo- lis, Minnesota. Association for Computational Linguistics. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy- ong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. A survey on in-context learning.",
    "chunk_size": 973,
    "word_count": 141,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 54,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_54",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "Association for Computational Linguistics. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy- ong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. A survey on in-context learning. Martin Fajcik, Martin Docekal, Karel Ondrej, and Pavel Smrz. 2021. R2-D2: A modular baseline for open-domain question answering. In Find- ings of the Association for Computational Lin- guistics: EMNLP 2021 , pages 854 870, Punta Cana, Dominican Republic. Association for Computational Linguistics. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2021. The pile: An 800gb dataset of diverse text for lan- guage modeling. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. REALM: Retrieval-augmented language model pre- training. In ICML. Junxian He, Graham Neubig, and Taylor Berg- Kirkpatrick. 2021. Efficient nearest neighbor language models.",
    "chunk_size": 992,
    "word_count": 146,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 55,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_55",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "and Ming-Wei Chang. 2020. REALM: Retrieval-augmented language model pre- training. In ICML. Junxian He, Graham Neubig, and Taylor Berg- Kirkpatrick. 2021. Efficient nearest neighbor language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 5703 5714, Online and Punta Cana, Dominican Republic. Associa- tion for Computational Linguistics. Minlie Huang, Xiaoyan Zhu, and Jianfeng Gao. 2020. Challenges in building intelligent open- domain dialog systems. ACM Trans. Inf. Syst. , 38(3). Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, ArmandJoulin, and Edouard Grave. 2022a. Unsu- pervised dense information retrieval with con- trastive learning. Transactions on Machine Learning Research. Gautier Izacard and Edouard Grave. 2021. Lever- aging passage retrieval with generative models for open domain question answering.",
    "chunk_size": 912,
    "word_count": 123,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 56,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_56",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "con- trastive learning. Transactions on Machine Learning Research. Gautier Izacard and Edouard Grave. 2021. Lever- aging passage retrieval with generative models for open domain question answering. In Pro- ceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pages 874 880, On- line. Association for Computational Linguistics. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lu- cas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022b. Atlas: Few-shot learning with retrieval augmented language mod- els. Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2021. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data , 7(3):535 547. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for read- ing comprehension.",
    "chunk_size": 932,
    "word_count": 130,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 57,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_57",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "on Big Data , 7(3):535 547. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for read- ing comprehension. In Proceedings of the 55th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers) , pages 1601 1611, Vancouver, Canada. Associa- tion for Computational Linguistics. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage re- trieval for open-domain question answering. In Proceedings of the 2020 Conference on Empir- ical Methods in Natural Language Processing (EMNLP) , pages 6769 6781, Online. Associa- tion for Computational Linguistics. Urvashi Khandelwal, Omer Levy, Dan Juraf- sky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations.",
    "chunk_size": 956,
    "word_count": 134,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 58,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_58",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "Omer Levy, Dan Juraf- sky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, An- drew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A bench- mark for question answering research. Trans- actions of the Association for Computational Linguistics , 7:452 466. Yoav Levine, Itay Dalmedigos, Ori Ram, Yoel Zeldes, Daniel Jannai, Dor Muhlgay, Yoni Osin, Opher Lieber, Barak Lenz, Shai Shalev-Shwartz, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2022a. Standing on the shoul- ders of giant frozen language models.",
    "chunk_size": 889,
    "word_count": 122,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 59,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_59",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "Jannai, Dor Muhlgay, Yoni Osin, Opher Lieber, Barak Lenz, Shai Shalev-Shwartz, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2022a. Standing on the shoul- ders of giant frozen language models. Yoav Levine, Ori Ram, Daniel Jannai, Barak Lenz, Shai Shalev-Shwartz, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2022b. Huge frozen language models as readers for open- domain question answering. In ICML 2022 Workshop on Knowledge Retrieval and Lan- guage Models. Yoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen, and Amnon Shashua. 2022c. The inductive bias of in-context learn- ing: Rethinking pretraining example design. In International Conference on Learning Represen- tations. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented gen- eration for knowledge-intensive nlp tasks.",
    "chunk_size": 968,
    "word_count": 134,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 60,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_60",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented gen- eration for knowledge-intensive nlp tasks. In Advances in Neural Information Processing Sys- tems, pages 9459 9474. Zonglin Li, Ruiqi Guo, and Sanjiv Kumar. 2022. Decoupled context processing for context aug- mented language modeling. In Advances in Neu- ral Information Processing Systems. Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. 2021. Jurassic-1: Technical details and evaluation. Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: A python toolkit for reproducible information retrieval research with sparse and dense representations. In Proceed- ings of the 44th International ACM SIGIR Con- ference on Research and Development in Infor- mation Retrieval , SIGIR '21, page 2356 2362, New York, NY , USA. Association for Comput- ing Machinery.",
    "chunk_size": 970,
    "word_count": 138,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 61,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_61",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "ings of the 44th International ACM SIGIR Con- ference on Research and Development in Infor- mation Retrieval , SIGIR '21, page 2356 2362, New York, NY , USA. Association for Comput- ing Machinery. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers) , pages 3214 3252, Dublin, Ireland. Association for Computational Linguistics. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized bert pretraining approach. Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Pro- ceedings of the 58th Annual Meeting of the As- sociation for Computational Linguistics , pages 1906 1919, Online. Association for Computa- tional Linguistics.",
    "chunk_size": 998,
    "word_count": 147,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 62,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_62",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "in abstractive summarization. In Pro- ceedings of the 58th Annual Meeting of the As- sociation for Computational Linguistics , pages 1906 1919, Online. Association for Computa- tional Linguistics. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mix- ture models. Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov, Omri Abend, Kevin Leyton-Brown, Amnon Shashua, and Yoav Shoham. 2023. Generating benchmarks for factuality evaluation of language models. Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021. KILT: a benchmark for knowledge intensive lan- guage tasks. In Proceedings of the 2021 Con- ference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies , pages 2523 2544, Online.",
    "chunk_size": 979,
    "word_count": 138,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 63,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_63",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "lan- guage tasks. In Proceedings of the 2021 Con- ference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies , pages 2523 2544, Online. Association for Computational Linguis- tics. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Lan- guage models are unsupervised multitask learn- ers. Ori Ram, Gal Shachaf, Omer Levy, Jonathan Be- rant, and Amir Globerson. 2022. Learning to re- trieve passages without supervision. In Proceed- ings of the 2022 Conference of the North Amer- ican Chapter of the Association for Computa- tional Linguistics: Human Language Technolo- gies, pages 2687 2700, Seattle, United States. Association for Computational Linguistics.",
    "chunk_size": 889,
    "word_count": 127,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 64,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_64",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "of the North Amer- ican Chapter of the Association for Computa- tional Linguistics: Human Language Technolo- gies, pages 2687 2700, Seattle, United States. Association for Computational Linguistics. Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Omri Abend, Ehud Karpas, Am- non Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2022. Parallel context windows im- prove in-context learning of large language mod- els. Stephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: BM25 and beyond. Found. Trends Inf. Retr. , 3(4):333 389. Devendra Sachan, Mike Lewis, Mandar Joshi, Ar- men Aghajanyan, Wen-tau Yih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving passage retrieval with zero-shot question generation. In Proceedings of the 2022 Conference on Empir- ical Methods in Natural Language Processing , pages 3781 3797, Abu Dhabi, United Arab Emi- rates. Association for Computational Linguis- tics.",
    "chunk_size": 932,
    "word_count": 133,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 65,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_65",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "In Proceedings of the 2022 Conference on Empir- ical Methods in Natural Language Processing , pages 3781 3797, Abu Dhabi, United Arab Emi- rates. Association for Computational Linguis- tics. Weijia Shi, Sewon Min, Michihiro Yasunaga, Min- joon Seo, Rich James, Mike Lewis, Luke Zettle- moyer, and Wen tau Yih. 2023. REPLUG: Retrieval-augmented black-box language mod- els. Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Proceedings of the Neural Information Process- ing Systems Track on Datasets and Benchmarks , volume 1. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and efficient foun- dation language models.",
    "chunk_size": 957,
    "word_count": 135,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 66,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_66",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and efficient foun- dation language models. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez,Ł ukasz Kaiser, and Illia Polosukhin. 2017. At- tention is all you need. In Advances in Neural Information Processing Systems 30 , pages 5998  6008. Ben Wang and Aran Komatsuzaki. 2021. GPT-J- 6B: A 6 Billion Parameter Autoregressive Lan- guage Model. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexan- der Rush. 2020. Transformers: State-of-the-art natural language processing.",
    "chunk_size": 920,
    "word_count": 132,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 67,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_67",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexan- der Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demon- strations , pages 38 45, Online. Association for Computational Linguistics. Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. In Advances in Neural Information Processing Systems , volume 32. Curran Asso- ciates, Inc. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, An- jali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open pre-trained transformer lan- guage models. Zexuan Zhong, Tao Lei, and Danqi Chen.",
    "chunk_size": 996,
    "word_count": 144,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 68,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_68",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "Kurt Shuster, Daniel Simig, Punit Singh Koura, An- jali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open pre-trained transformer lan- guage models. Zexuan Zhong, Tao Lei, and Danqi Chen. 2022. Training language models with memory augmen- tation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Pro- cessing , pages 5657 5673, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. A Query Length Ablations Figure 9 and Figure 10 show ablations on the opti- mal query length ℓfor off-the-shelf dense retrievers (BERT and Contriever respectively). We omit the results of Spider as they are almost identical to those of Contriever. Consistently, using ℓ  64 (tokens) is optimal. This is in contrast to similar experiments we conducted for BM25 ( cf. Figure 6), where ℓ  32 is optimal. B GPT-Neo Results Table 5 gives the results of applying In-Context RALM to the models from the GPT-Neo model family on WikiText-103 and RealNews.",
    "chunk_size": 990,
    "word_count": 156,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 69,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_69",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "for BM25 ( cf. Figure 6), where ℓ  32 is optimal. B GPT-Neo Results Table 5 gives the results of applying In-Context RALM to the models from the GPT-Neo model family on WikiText-103 and RealNews. C Open-Domain Question Answering Experiments: Further Details Closed-Book Setting For the closed-book set- ting, we adopt the prompt of Touvron et al. (2023): Answer these questions: Q: Who got the first nobel prize in physics. A: Open-Book Setting For the open-book setting, we extend the above prompt as follows: Nobel Prize A group including 42 Swedish writers, artists, and literary critics protested against this decision, having expected Leo Tolstoy to be awarded. Some, including Burton Feldman, have criticised this prize because they. Nobel Prize in Physiology or Medicine In the last half century there has been an increasing tendency for scientists to work as teams, resulting in controversial exclusions.",
    "chunk_size": 912,
    "word_count": 144,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 70,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_70",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "this prize because they. Nobel Prize in Physiology or Medicine In the last half century there has been an increasing tendency for scientists to work as teams, resulting in controversial exclusions. Alfred Nobel was born on 21 October 1833 in Stockholm, Sweden, into a family of engineers. Based on these texts, answer these questions: Q: Who got the first nobel prize in physics. A:Model RetrievalWiki-103 RealNews word ppl token ppl GPT-Neo 1. 3B- 17. 5 12. 3 BM25,  5 14. 6 9. 9 GPT-Neo 2. 7B- 15. 1 11. O BM25,  5 12. 8 9. O GPT-J 6B- 11. 6 9. 2 BM25,  5 10. O 7. 7 Table 5: The performance of models from the GPT- Neo family, measured by word-level perplexity on the test set of WikiText-103 and token-level per- plexity on the development set of RealNews. Retrieval Query Length (ℓ)Perplexity 15. 025. 035. 045. O 16 32 64 128 256GPT-2 117M (S) GPT-2 345M (M) GPT-2 762M (I) GPT-2 1.",
    "chunk_size": 888,
    "word_count": 163,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 71,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_71",
    "created_at": "2025-07-22T23:16:55.607971"
  },
  {
    "text": "and token-level per- plexity on the development set of RealNews. Retrieval Query Length (ℓ)Perplexity 15. 025. 035. 045. O 16 32 64 128 256GPT-2 117M (S) GPT-2 345M (M) GPT-2 762M (I) GPT-2 1. 5B (XL) Figure 9: An analysis of perplexity as a function ofthe number of tokens in the query for an off- the-shelf BERT retriever on the development set of WikiText-103. Retrieval Query Length (ℓ)Perplexity 10. 020. 030. 040. O 16 32 64 128 256GPT-2 117M (S) GPT-2 345M (M) GPT-2 762M (I) GPT-2 1. 5B (XL) Figure 10: An analysis of perplexity as a function ofthe number of tokens in the query for Contriever on the development set of WikiText-103.",
    "chunk_size": 641,
    "word_count": 114,
    "document_id": "2302.00083-In-Context Retrieval-Augmented Language Models",
    "chunk_index": 72,
    "chunk_id": "2302.00083-In-Context Retrieval-Augmented Language Models_chunk_72",
    "created_at": "2025-07-22T23:16:55.607971"
  }
]